{
 "metadata": {
  "name": "",
  "signature": "sha256:c462273596e4a0cb6c8606b9e5f88d1369aa7b9db9790adb37eda34305f94839"
 },
 "nbformat": 3,
 "nbformat_minor": 0,
 "worksheets": [
  {
   "cells": [
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "from IPython.core.display import HTML\n",
      "def css_styling():\n",
      "    styles = open(\"./styles/custom.css\", \"r\").read()\n",
      "    return HTML(styles)\n",
      "css_styling()"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "html": [
        "<style>\n",
        "    @font-face {\n",
        "        font-family: \"Computer Modern\";\n",
        "        src: url('http://9dbb143991406a7c655e-aa5fcb0a5a4ec34cff238a2d56ca4144.r56.cf5.rackcdn.com/cmunss.otf');\n",
        "    }\n",
        "    @font-face {\n",
        "        font-family: \"Computer Modern\";\n",
        "        font-weight: bold;\n",
        "        src: url('http://9dbb143991406a7c655e-aa5fcb0a5a4ec34cff238a2d56ca4144.r56.cf5.rackcdn.com/cmunsx.otf');\n",
        "    }\n",
        "    @font-face {\n",
        "        font-family: \"Computer Modern\";\n",
        "        font-style: oblique;\n",
        "        src: url('http://9dbb143991406a7c655e-aa5fcb0a5a4ec34cff238a2d56ca4144.r56.cf5.rackcdn.com/cmunsi.otf');\n",
        "    }\n",
        "    @font-face {\n",
        "        font-family: \"Computer Modern\";\n",
        "        font-weight: bold;\n",
        "        font-style: oblique;\n",
        "        src: url('http://9dbb143991406a7c655e-aa5fcb0a5a4ec34cff238a2d56ca4144.r56.cf5.rackcdn.com/cmunso.otf');\n",
        "    }\n",
        "    div.cell{\n",
        "        width:800px;\n",
        "        margin-left:16% !important;\n",
        "        margin-right:auto;\n",
        "    }\n",
        "    h1 {\n",
        "        font-family: Helvetica, serif;\n",
        "    }\n",
        "    h4{\n",
        "        margin-top:12px;\n",
        "        margin-bottom: 3px;\n",
        "       }\n",
        "    div.text_cell_render{\n",
        "        font-family: Computer Modern, \"Helvetica Neue\", Arial, Helvetica, Geneva, sans-serif;\n",
        "        line-height: 145%;\n",
        "        font-size: 130%;\n",
        "        width:800px;\n",
        "        margin-left:auto;\n",
        "        margin-right:auto;\n",
        "    }\n",
        "    .CodeMirror{\n",
        "            font-family: \"Source Code Pro\", source-code-pro,Consolas, monospace;\n",
        "    }\n",
        "    .prompt{\n",
        "        display: None;\n",
        "    }\n",
        "    .text_cell_render h5 {\n",
        "        font-weight: 300;\n",
        "        font-size: 22pt;\n",
        "        color: #4057A1;\n",
        "        font-style: italic;\n",
        "        margin-bottom: .5em;\n",
        "        margin-top: 0.5em;\n",
        "        display: block;\n",
        "    }\n",
        "    \n",
        "    .warning{\n",
        "        color: rgb( 240, 20, 20 )\n",
        "        }  \n",
        "</style>\n",
        "<script>\n",
        "    MathJax.Hub.Config({\n",
        "                        TeX: {\n",
        "                           extensions: [\"AMSmath.js\"]\n",
        "                           },\n",
        "                tex2jax: {\n",
        "                    inlineMath: [ ['$','$'], [\"\\\\(\",\"\\\\)\"] ],\n",
        "                    displayMath: [ ['$$','$$'], [\"\\\\[\",\"\\\\]\"] ]\n",
        "                },\n",
        "                displayAlign: 'center', // Change this to 'center' to center equations.\n",
        "                \"HTML-CSS\": {\n",
        "                    styles: {'.MathJax_Display': {\"margin\": 4}}\n",
        "                }\n",
        "        });\n",
        "</script>"
       ],
       "metadata": {},
       "output_type": "pyout",
       "prompt_number": 1,
       "text": [
        "<IPython.core.display.HTML at 0x1021c4610>"
       ]
      }
     ],
     "prompt_number": 1
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "%load_ext gvmagic"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": 19
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "#Chapter 1: Parameter Estimation"
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "%%dot digraph G {\n",
      "    \"Distribution\" -> \"Probability Density Function\"\n",
      "    \"Distribution\" -> \"Probability Mass Function\"\n",
      "    \"Variance-Covariance\" \n",
      "    \"Positive Definite\" \n",
      "    \"Postive Semi Definite\" -> \"Non negative definite\"\n",
      "    \"Distribution\" -> \"Parameter Estimation\"\n",
      "    \"Parameter Estimation\" -> \"MLE\"\n",
      "    \"Parameter Estimation\" -> \"MAP\"\n",
      "    \"Parameter Estimation\" -> \"Bayesian Estimate\"\n",
      "    \"Bayesian Estimate\" -> \"Theory\"\n",
      "    \"Bayesian Estimate\" -> \"Practicse\"\n",
      "    \"Conjugate Prior\" -> \"Theory\"\n",
      "    \"Monte Carlo Method\" -> \"Practicse\"\n",
      "    \n",
      "    \n",
      "\n",
      "}"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "metadata": {},
       "output_type": "display_data",
       "svg": [
        "<?xml version=\"1.0\" encoding=\"UTF-8\" standalone=\"no\"?>\n",
        "<!DOCTYPE svg PUBLIC \"-//W3C//DTD SVG 1.1//EN\"\n",
        " \"http://www.w3.org/Graphics/SVG/1.1/DTD/svg11.dtd\">\n",
        "<!-- Generated by graphviz version 2.38.0 (20140413.2041)\n",
        " -->\n",
        "<!-- Title: G Pages: 1 -->\n",
        "<svg width=\"1101pt\" height=\"260pt\"\n",
        " viewBox=\"0.00 0.00 1100.56 260.00\" xmlns=\"http://www.w3.org/2000/svg\" xmlns:xlink=\"http://www.w3.org/1999/xlink\">\n",
        "<g id=\"graph0\" class=\"graph\" transform=\"scale(1 1) rotate(0) translate(4 256)\">\n",
        "<title>G</title>\n",
        "<polygon fill=\"white\" stroke=\"none\" points=\"-4,4 -4,-256 1096.56,-256 1096.56,4 -4,4\"/>\n",
        "<!-- Distribution -->\n",
        "<g id=\"node1\" class=\"node\"><title>Distribution</title>\n",
        "<ellipse fill=\"none\" stroke=\"black\" cx=\"347.787\" cy=\"-234\" rx=\"52.7038\" ry=\"18\"/>\n",
        "<text text-anchor=\"middle\" x=\"347.787\" y=\"-229.8\" font-family=\"Times,serif\" font-size=\"14.00\">Distribution</text>\n",
        "</g>\n",
        "<!-- Probability Density Function -->\n",
        "<g id=\"node2\" class=\"node\"><title>Probability Density Function</title>\n",
        "<ellipse fill=\"none\" stroke=\"black\" cx=\"112.787\" cy=\"-162\" rx=\"112.574\" ry=\"18\"/>\n",
        "<text text-anchor=\"middle\" x=\"112.787\" y=\"-157.8\" font-family=\"Times,serif\" font-size=\"14.00\">Probability Density Function</text>\n",
        "</g>\n",
        "<!-- Distribution&#45;&gt;Probability Density Function -->\n",
        "<g id=\"edge1\" class=\"edge\"><title>Distribution&#45;&gt;Probability Density Function</title>\n",
        "<path fill=\"none\" stroke=\"black\" d=\"M309.245,-221.519C272.4,-210.544 216.276,-193.827 173.437,-181.066\"/>\n",
        "<polygon fill=\"black\" stroke=\"black\" points=\"174.266,-177.661 163.683,-178.16 172.268,-184.37 174.266,-177.661\"/>\n",
        "</g>\n",
        "<!-- Probability Mass Function -->\n",
        "<g id=\"node3\" class=\"node\"><title>Probability Mass Function</title>\n",
        "<ellipse fill=\"none\" stroke=\"black\" cx=\"347.787\" cy=\"-162\" rx=\"103.896\" ry=\"18\"/>\n",
        "<text text-anchor=\"middle\" x=\"347.787\" y=\"-157.8\" font-family=\"Times,serif\" font-size=\"14.00\">Probability Mass Function</text>\n",
        "</g>\n",
        "<!-- Distribution&#45;&gt;Probability Mass Function -->\n",
        "<g id=\"edge2\" class=\"edge\"><title>Distribution&#45;&gt;Probability Mass Function</title>\n",
        "<path fill=\"none\" stroke=\"black\" d=\"M347.787,-215.697C347.787,-207.983 347.787,-198.712 347.787,-190.112\"/>\n",
        "<polygon fill=\"black\" stroke=\"black\" points=\"351.287,-190.104 347.787,-180.104 344.287,-190.104 351.287,-190.104\"/>\n",
        "</g>\n",
        "<!-- Parameter Estimation -->\n",
        "<g id=\"node8\" class=\"node\"><title>Parameter Estimation</title>\n",
        "<ellipse fill=\"none\" stroke=\"black\" cx=\"555.787\" cy=\"-162\" rx=\"86.4621\" ry=\"18\"/>\n",
        "<text text-anchor=\"middle\" x=\"555.787\" y=\"-157.8\" font-family=\"Times,serif\" font-size=\"14.00\">Parameter Estimation</text>\n",
        "</g>\n",
        "<!-- Distribution&#45;&gt;Parameter Estimation -->\n",
        "<g id=\"edge4\" class=\"edge\"><title>Distribution&#45;&gt;Parameter Estimation</title>\n",
        "<path fill=\"none\" stroke=\"black\" d=\"M384.018,-220.807C416.819,-209.768 465.551,-193.368 502.739,-180.853\"/>\n",
        "<polygon fill=\"black\" stroke=\"black\" points=\"504.051,-184.104 512.413,-177.597 501.819,-177.47 504.051,-184.104\"/>\n",
        "</g>\n",
        "<!-- Variance&#45;Covariance -->\n",
        "<g id=\"node4\" class=\"node\"><title>Variance&#45;Covariance</title>\n",
        "<ellipse fill=\"none\" stroke=\"black\" cx=\"502.787\" cy=\"-234\" rx=\"84.4979\" ry=\"18\"/>\n",
        "<text text-anchor=\"middle\" x=\"502.787\" y=\"-229.8\" font-family=\"Times,serif\" font-size=\"14.00\">Variance&#45;Covariance</text>\n",
        "</g>\n",
        "<!-- Positive Definite -->\n",
        "<g id=\"node5\" class=\"node\"><title>Positive Definite</title>\n",
        "<ellipse fill=\"none\" stroke=\"black\" cx=\"674.787\" cy=\"-234\" rx=\"69.1164\" ry=\"18\"/>\n",
        "<text text-anchor=\"middle\" x=\"674.787\" y=\"-229.8\" font-family=\"Times,serif\" font-size=\"14.00\">Positive Definite</text>\n",
        "</g>\n",
        "<!-- Postive Semi Definite -->\n",
        "<g id=\"node6\" class=\"node\"><title>Postive Semi Definite</title>\n",
        "<ellipse fill=\"none\" stroke=\"black\" cx=\"849.787\" cy=\"-234\" rx=\"87.4827\" ry=\"18\"/>\n",
        "<text text-anchor=\"middle\" x=\"849.787\" y=\"-229.8\" font-family=\"Times,serif\" font-size=\"14.00\">Postive Semi Definite</text>\n",
        "</g>\n",
        "<!-- Non negative definite -->\n",
        "<g id=\"node7\" class=\"node\"><title>Non negative definite</title>\n",
        "<ellipse fill=\"none\" stroke=\"black\" cx=\"849.787\" cy=\"-162\" rx=\"86.4791\" ry=\"18\"/>\n",
        "<text text-anchor=\"middle\" x=\"849.787\" y=\"-157.8\" font-family=\"Times,serif\" font-size=\"14.00\">Non negative definite</text>\n",
        "</g>\n",
        "<!-- Postive Semi Definite&#45;&gt;Non negative definite -->\n",
        "<g id=\"edge3\" class=\"edge\"><title>Postive Semi Definite&#45;&gt;Non negative definite</title>\n",
        "<path fill=\"none\" stroke=\"black\" d=\"M849.787,-215.697C849.787,-207.983 849.787,-198.712 849.787,-190.112\"/>\n",
        "<polygon fill=\"black\" stroke=\"black\" points=\"853.287,-190.104 849.787,-180.104 846.287,-190.104 853.287,-190.104\"/>\n",
        "</g>\n",
        "<!-- MLE -->\n",
        "<g id=\"node9\" class=\"node\"><title>MLE</title>\n",
        "<ellipse fill=\"none\" stroke=\"black\" cx=\"479.787\" cy=\"-90\" rx=\"28.5497\" ry=\"18\"/>\n",
        "<text text-anchor=\"middle\" x=\"479.787\" y=\"-85.8\" font-family=\"Times,serif\" font-size=\"14.00\">MLE</text>\n",
        "</g>\n",
        "<!-- Parameter Estimation&#45;&gt;MLE -->\n",
        "<g id=\"edge5\" class=\"edge\"><title>Parameter Estimation&#45;&gt;MLE</title>\n",
        "<path fill=\"none\" stroke=\"black\" d=\"M537.389,-144.055C527.067,-134.548 514.087,-122.593 503.063,-112.439\"/>\n",
        "<polygon fill=\"black\" stroke=\"black\" points=\"505.174,-109.625 495.448,-105.424 500.432,-114.774 505.174,-109.625\"/>\n",
        "</g>\n",
        "<!-- MAP -->\n",
        "<g id=\"node10\" class=\"node\"><title>MAP</title>\n",
        "<ellipse fill=\"none\" stroke=\"black\" cx=\"555.787\" cy=\"-90\" rx=\"29.0514\" ry=\"18\"/>\n",
        "<text text-anchor=\"middle\" x=\"555.787\" y=\"-85.8\" font-family=\"Times,serif\" font-size=\"14.00\">MAP</text>\n",
        "</g>\n",
        "<!-- Parameter Estimation&#45;&gt;MAP -->\n",
        "<g id=\"edge6\" class=\"edge\"><title>Parameter Estimation&#45;&gt;MAP</title>\n",
        "<path fill=\"none\" stroke=\"black\" d=\"M555.787,-143.697C555.787,-135.983 555.787,-126.712 555.787,-118.112\"/>\n",
        "<polygon fill=\"black\" stroke=\"black\" points=\"559.287,-118.104 555.787,-108.104 552.287,-118.104 559.287,-118.104\"/>\n",
        "</g>\n",
        "<!-- Bayesian Estimate -->\n",
        "<g id=\"node11\" class=\"node\"><title>Bayesian Estimate</title>\n",
        "<ellipse fill=\"none\" stroke=\"black\" cx=\"829.787\" cy=\"-90\" rx=\"75.8547\" ry=\"18\"/>\n",
        "<text text-anchor=\"middle\" x=\"829.787\" y=\"-85.8\" font-family=\"Times,serif\" font-size=\"14.00\">Bayesian Estimate</text>\n",
        "</g>\n",
        "<!-- Parameter Estimation&#45;&gt;Bayesian Estimate -->\n",
        "<g id=\"edge7\" class=\"edge\"><title>Parameter Estimation&#45;&gt;Bayesian Estimate</title>\n",
        "<path fill=\"none\" stroke=\"black\" d=\"M608.293,-147.586C654.668,-135.738 722.235,-118.477 770.532,-106.138\"/>\n",
        "<polygon fill=\"black\" stroke=\"black\" points=\"771.404,-109.528 780.226,-103.661 769.671,-102.746 771.404,-109.528\"/>\n",
        "</g>\n",
        "<!-- Theory -->\n",
        "<g id=\"node12\" class=\"node\"><title>Theory</title>\n",
        "<ellipse fill=\"none\" stroke=\"black\" cx=\"765.787\" cy=\"-18\" rx=\"35.7887\" ry=\"18\"/>\n",
        "<text text-anchor=\"middle\" x=\"765.787\" y=\"-13.8\" font-family=\"Times,serif\" font-size=\"14.00\">Theory</text>\n",
        "</g>\n",
        "<!-- Bayesian Estimate&#45;&gt;Theory -->\n",
        "<g id=\"edge8\" class=\"edge\"><title>Bayesian Estimate&#45;&gt;Theory</title>\n",
        "<path fill=\"none\" stroke=\"black\" d=\"M814.294,-72.055C806.121,-63.1153 795.969,-52.0116 787.068,-42.2762\"/>\n",
        "<polygon fill=\"black\" stroke=\"black\" points=\"789.59,-39.848 780.26,-34.8294 784.424,-44.5714 789.59,-39.848\"/>\n",
        "</g>\n",
        "<!-- Practicse -->\n",
        "<g id=\"node13\" class=\"node\"><title>Practicse</title>\n",
        "<ellipse fill=\"none\" stroke=\"black\" cx=\"893.787\" cy=\"-18\" rx=\"42.0524\" ry=\"18\"/>\n",
        "<text text-anchor=\"middle\" x=\"893.787\" y=\"-13.8\" font-family=\"Times,serif\" font-size=\"14.00\">Practicse</text>\n",
        "</g>\n",
        "<!-- Bayesian Estimate&#45;&gt;Practicse -->\n",
        "<g id=\"edge9\" class=\"edge\"><title>Bayesian Estimate&#45;&gt;Practicse</title>\n",
        "<path fill=\"none\" stroke=\"black\" d=\"M845.28,-72.055C853.336,-63.243 863.315,-52.3285 872.124,-42.6943\"/>\n",
        "<polygon fill=\"black\" stroke=\"black\" points=\"874.713,-45.049 878.878,-35.307 869.547,-40.3256 874.713,-45.049\"/>\n",
        "</g>\n",
        "<!-- Conjugate Prior -->\n",
        "<g id=\"node14\" class=\"node\"><title>Conjugate Prior</title>\n",
        "<ellipse fill=\"none\" stroke=\"black\" cx=\"669.787\" cy=\"-90\" rx=\"66.2091\" ry=\"18\"/>\n",
        "<text text-anchor=\"middle\" x=\"669.787\" y=\"-85.8\" font-family=\"Times,serif\" font-size=\"14.00\">Conjugate Prior</text>\n",
        "</g>\n",
        "<!-- Conjugate Prior&#45;&gt;Theory -->\n",
        "<g id=\"edge10\" class=\"edge\"><title>Conjugate Prior&#45;&gt;Theory</title>\n",
        "<path fill=\"none\" stroke=\"black\" d=\"M692.053,-72.7646C705.758,-62.7711 723.437,-49.8798 738.049,-39.2256\"/>\n",
        "<polygon fill=\"black\" stroke=\"black\" points=\"740.226,-41.9697 746.244,-33.2499 736.102,-36.3136 740.226,-41.9697\"/>\n",
        "</g>\n",
        "<!-- Monte Carlo Method -->\n",
        "<g id=\"node15\" class=\"node\"><title>Monte Carlo Method</title>\n",
        "<ellipse fill=\"none\" stroke=\"black\" cx=\"1007.79\" cy=\"-90\" rx=\"84.5514\" ry=\"18\"/>\n",
        "<text text-anchor=\"middle\" x=\"1007.79\" y=\"-85.8\" font-family=\"Times,serif\" font-size=\"14.00\">Monte Carlo Method</text>\n",
        "</g>\n",
        "<!-- Monte Carlo Method&#45;&gt;Practicse -->\n",
        "<g id=\"edge11\" class=\"edge\"><title>Monte Carlo Method&#45;&gt;Practicse</title>\n",
        "<path fill=\"none\" stroke=\"black\" d=\"M981.346,-72.7646C964.767,-62.5843 943.29,-49.3969 925.757,-38.6306\"/>\n",
        "<polygon fill=\"black\" stroke=\"black\" points=\"927.347,-35.5 916.994,-33.2499 923.684,-41.4652 927.347,-35.5\"/>\n",
        "</g>\n",
        "</g>\n",
        "</svg>\n"
       ]
      }
     ],
     "prompt_number": 24
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "## 1.1 Motivating Example\n",
      "\n",
      "Assuming we are tasked with building a classifier that predicts gender (i.e \"male\" or \"female\") based on the hieght of the 20 year old person. One approach to build such a classifier is to use a  Bayesian approach. As per Bayesian approach, given certain height $(h)$, the probability of the person being male is given as:\n",
      "\n",
      "$$P(Male|h) = \\frac{P(h|Male) \\times P(Male)}{P(h)}~~~~~~(1)$$\n",
      "\n",
      "Similarily, probability of the same individual being female is:\n",
      "\n",
      "$$P(Female|h) = \\frac{P(h|Female) \\times P(Female)}{P(h)}$$\n",
      "\n",
      "Depending upon whether $P(Male|h) \\geq P(Female|h)$, we can classify the given person as Male or Female. To calculate $P(Male|h)$ and $P(Female|h)$ we need a way to estimate $P(Male)$, $P(Female)$, etc. One way to calculate these estimates is to use randomly sampled data. Let's assume we randomly surveyed 1000 youths in a given neighborhood/city and recorded individual's gender and his/her height. As discussed below, based on this randomly labeled data we can compute various aspects of the eqn 1. \n",
      "\n",
      "**1. Prior probabilities**\n",
      "\n",
      "Let's assume male and female populaion is equal. Without any other information if someone ask you to predict gender of a person then no whatever gender you choose you are likely to correct 50% of the time (think of this like a coin toss problem where you have to predict head or tail). However generally male and female population is not equally distributed. Let's assume of the 1000 people surveyed, 600 turned out to be male. Now let's assume that we are asked to come with a model to predict the gender of the person. There are three naive approaches to build such a model\n",
      "\n",
      "    1. Always predicting \"male\"\n",
      "    2. Always predicting \"Female\"\n",
      "    3. Randomly selecting one of the two categories\n",
      "\n",
      "Below we stimualate the three models and show that the best model is to always predict \"male\". This is because given just prior probabilities and no other information, it maximizes our chance to correctly predict gender. Statistically speaking we say that prior probability of \"male\" class ($P(Male) = {Number of Males}/{Population Size} = 600/1000 = 0.6$) is more than the prior probability of female class ($P(Female) = 0.4$) and hence predicting consistenly \"male\" will yield higher accuracy. \n"
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "# Assuming we sampled 1000 individiauls and 600 of them turned out to be male\n",
      "data = [\"M\"] * 600 + [\"F\"] * 400\n",
      "numSample = 10\n",
      "\n",
      "# We use random function to randomly select individuals from the above data. \n",
      "# We fix the seed so that we can reconstruct the sample data. However feel free \n",
      "# to try remove the seed and run the cell block multiple times to verify \n",
      "# that always predicting male yields best model. \n",
      "import random\n",
      "random.seed(1) \n",
      "\n",
      "# Randomly selecting 10 individuals from the above dataset\n",
      "sample = [data[random.randint(0, 1000)] for idx in range(numSample)] # Randomly select 10 individuals\n",
      "print \"Sampled data :\", sample\n",
      "print \"Number of Males sampled: \", sample.count(\"M\")\n",
      "print \"Number of Females sampled: \", sample.count(\"F\")\n",
      "\n",
      "\n",
      "#Calculate percentage accuracy\n",
      "def accuracy(actual, predicted):\n",
      "    s = 0\n",
      "    l = len(actual)\n",
      "    for i in range(l):\n",
      "        s += (1 if actual[i] == predicted[i] else 0)\n",
      "    return float(s)/l\n",
      "\n",
      "\n",
      "#Approach 1: Always predicting male\n",
      "intToClass = [\"M\", \"F\"]\n",
      "print \"\\nScenarios:\"\n",
      "print \"1. Always Male: \", accuracy(sample, [\"M\"] * numSample )\n",
      "print \"2. Always Female: \", accuracy(sample, [\"F\"] * numSample)\n",
      "print \"3. Random: \", accuracy(sample, [intToClass[random.randint(0, 1)] for i in range(numSample)])\n",
      "\n"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "Sampled data : ['M', 'F', 'F', 'M', 'M', 'M', 'F', 'F', 'M', 'M']\n",
        "Number of Males sampled:  6\n",
        "Number of Females sampled:  4\n",
        "\n",
        "Scenarios:\n",
        "1. Always Male:  0.6\n",
        "2. Always Female:  0.4\n",
        "3. Random:  0.5\n"
       ]
      }
     ],
     "prompt_number": 2
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "**2 Likelihood**\n",
      "\n",
      "Another important term in eqn 1. is the likelihood term i.e $P(h|Male)$. Likelihood indicates the probability of observing certain height $(h)$ given that its belongs to a Male. Basically the idea is that height of males and females follow certain distribution. Based on these distributions it possible to calculate the probability of a person associated with a given class based on his/her height. For instance, let's assume that the distribution of heights for both male and female is normally distributed. A normal distribution can be represented by two parameters: Mean ($\\mu$) and Standard Deviation ($\\sigma$). Generally all the parameters required to represent a distribution are clubed and represented by a single symbol $\\theta$. In school we have learnt how to calculate mean and variance given certain dataset. Mainly we have learnt that for normal distribution mean and variance is given as \n",
      "\n",
      "$$\\mu = \\sigma_{x_i}/N, \\sigma^2 = \\sigma_{(x_i - \\mu)^2}/N-1$$\n",
      "\n",
      "However, as we will discuss later, **the above computation of mean and variance make certain assumptions and these assumptions are the main point of contention that give rise to different parameter estimation techniques, namely: MAP, etc.**. \n",
      "\n",
      "But for now let's focus on likelihood and how it can help us with the classification problem. Coming back to the predicting gender problem, let's assume based on the our survey data we can estimate $\\theta_{Male}$ (i.e. mean and variance for the distribution of heights for the male class) and $\\theta_{Female}$. Table below shows some hypothetical mean and variance for the male and female class. \n",
      "\n",
      "|Gender|Mean (inch) | Standard Devation (inch) |\n",
      "|------|----|-------|--------------------------|\n",
      "| Male | 68 | 4 |\n",
      "| Female | 64 | 5 |\n",
      "\n",
      "\n",
      "Now let's assume we measured an individual's height to be 70 inches. As show in the figure below, If we plot the distribution of heights for male and female, we can easily see that the probability of a male having a height of 70 inches (i.e. $P(h|Male)$) is about 0.09. Whereas the probability of a female having a height of 70 inchdes is about 0.04. Thus based on liklihood we can classify the individual belonging to the \"Male\" class. Similarly we observe height of 55 inches then $P(h|Female) \\approx 0.0$ and $P(h|Male) \\approx 0.02$. Thus in this case we can classify the individual being as \"Female\""
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "%matplotlib inline\n",
      "import matplotlib.pyplot as plt\n",
      "import numpy as np\n",
      "import matplotlib.mlab as mlab\n",
      "import math\n",
      "\n",
      "# Define distribution parameters for Male & Female class\n",
      "theta = {'Male': (68,4), 'Female': (64,5)} \n",
      "\n",
      "# Plot distribution\n",
      "x = np.linspace(40,100,100)\n",
      "for cls, params in theta.items():\n",
      "    plt.plot(x, mlab.normpdf(x, *params), label=cls)\n",
      "# plt.plot(x,mlab.normpdf(x,68,4), label=\"Male\")  # Plot Male Distribution\n",
      "# plt.plot(x,mlab.normpdf(x,64,5), label=\"Female\") # Plot Female Distribution\n",
      "\n",
      "# Plot 70 inches vline and annotate with p(h|Male) & p(h|Female)\n",
      "observed = 70\n",
      "plt.axvline(observed, color='black', linestyle=\"dashed\")  # Plot 70 inch marker\n",
      "for cls, params in theta.items():\n",
      "    prob = mlab.normpdf(observed, *params)\n",
      "    plt.text(observed+1, prob, \"%.2f\" % prob)\n",
      "\n",
      "# Display legend \n",
      "plt.legend()\n",
      "plt.show()"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "metadata": {},
       "output_type": "display_data",
       "png": "iVBORw0KGgoAAAANSUhEUgAAAX4AAAEACAYAAAC08h1NAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\n1YwEfiGEqGb+P5LpmRQ6dwf9AAAAAElFTkSuQmCC\n",
       "text": [
        "<matplotlib.figure.Figure at 0x103969ed0>"
       ]
      }
     ],
     "prompt_number": 3
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "Now that we have prior probability and likelihood, we can plug these values in eqn 1 to get $P(Male|h)$ and $P(Female|h)$. Note that we ignore $P(h)$ as its just a normalizing constant. Let's put all the above discussion in a code. "
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "import matplotlib.mlab as mlab\n",
      "\n",
      "# classes:\n",
      "classes = ['Male', 'Female']\n",
      "\n",
      "# Prior probabilities\n",
      "prior = {'Male': 0.6, 'Female': 0.4}\n",
      "\n",
      "# Mean and SD \n",
      "theta = {'Male': (68,4), 'Female': (64,5)} \n",
      "\n",
      "\n",
      "# Classify individuals with height ranging from 61 to 67\n",
      "for observed in range(61, 67):\n",
      "        estimates = []\n",
      "        #calculate P(_|h) for each class\n",
      "        \n",
      "        for cls in classes:\n",
      "            prob = prior[cls] * mlab.normpdf(observed, *theta[cls])\n",
      "            estimates.append((cls, prob))\n",
      "        \n",
      "        # Sort estimates by value in descending order and pick top most class\n",
      "        print observed, sorted(estimates, key=lambda x: -1 * x[1])[0][0]\n",
      "            \n"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "61 Female\n",
        "62 Female\n",
        "63 Female\n",
        "64 Male\n",
        "65 Male\n",
        "66 Male\n"
       ]
      }
     ],
     "prompt_number": 4
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "From above we can see that the decision boundary for our class is at 63 inchdes. For any height less than 63 inches our classifier essentially classify an individual as Female and any height more than 63 inches as male. We can further visualize this by plot the joint distribution $P(male,h)$ and $P(female,h)$ (i.e. numerator of eqn 1). "
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "%matplotlib inline\n",
      "import matplotlib.pyplot as plt\n",
      "import numpy as np\n",
      "import matplotlib.mlab as mlab\n",
      "import math\n",
      "\n",
      "prior = {'Male': 0.6, 'Female': 0.4} # Prior probability\n",
      "theta = {'Male': (68,4), 'Female': (64,5)}  # Distribution Parameters\n",
      "\n",
      "# Plot distribution\n",
      "x = np.linspace(40,100,100)\n",
      "for cls, params in theta.items():\n",
      "    plt.plot(x, prior[cls] * mlab.normpdf(x, *params), label=cls)\n",
      "    \n",
      "plt.axvline(64, linestyle=\"dashed\", color=\"black\") # plot decision boundary\n",
      "\n",
      "# Display legend \n",
      "plt.legend()\n",
      "plt.show()"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "metadata": {},
       "output_type": "display_data",
       "png": "iVBORw0KGgoAAAANSUhEUgAAAX4AAAEACAYAAAC08h1NAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\nAAAAAElFTkSuQmCC\n",
       "text": [
        "<matplotlib.figure.Figure at 0x103a66cd0>"
       ]
      }
     ],
     "prompt_number": 5
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "##1.2 Fundamentals\n",
      "\n",
      "##1.2.1 What is Probability Distribution Function\n",
      "In the above example we had only one input feature, namely height. Since height is a real value, we had 1 dimensional real value space. Further we assumed that the height is normally distributed and calculated its mean and variance in order to calculate $P(h|\\theta_{Male})$. \n",
      "\n",
      "A real world classifier however will take many more input features such as height, weight, age, waist-size, arm-length, etc as inputs to our gender predicting classifier. Thus we have n-dimensional real value space where n represents number of input features. Let's represent these input features for a single feature as $\\underline{x}$. Underline here indicates that x its a vector. \n",
      "\n",
      "To predict whether a person is a male or female then we need a function that convert that this set of input features $\\underline{x}$ into some probability value between 0 and 1 for male and female classes. (i.e. we need some function to calculate $P(\\underline{x}|\\theta_{Male})$ and $P(\\underline{x}|\\theta_{Female})$). **The function that converts a n-dimensional real value into another real value that ranges between 0 and 1 is called probability distribution function (PDF)**. \n",
      "\n",
      "A PDF satisfies following two conditions:\n",
      "\n",
      "1. $pdf(x) \\geq 0 ~~\\forall~x~\\epsilon~\\mathbb{R}^n$ i.e. the function for any n-dimensional real value should return value greater than 0. \n",
      "2. $\\int_{\\mathbb{R}^2}p(x)dx = 1$ i.e. area of under the curve bounded by PDF should be equal to 1.\n",
      "\n",
      "Depending on the nature of the input features, PDFs are sometimes differentiated into **probability mass function and probability density function**. If the input feature is a discrete variable then the probability distribution function is called as **probability mass function** (pmt). It gives probability of a random variable being exactly some real value. In contrast if the input feature can take continuous values then the probability distribution function is called as **probability density function** (pdf). \n",
      "\n",
      "Also note that **probability density function is different from probability distribution function**. Probability density function gives probability of a random variable having some specific value whereas probability distribution function is a cumulative probability of having some random variable $X$ having value $\\leq x$. i.e.\n",
      "\n",
      "$$pdf  = p(X=x)$$\n",
      "$$PDF=\\int_{-\\infty}^{x}p(x)dx$$\n",
      "\n",
      "\n",
      "In summary:\n",
      "\n",
      "* Probability Distribution Function -- answers what is the probability of X being less than or equal to x\n",
      "* Probability Mass Function -- answer what is the probability of X being exactly x in the case of discrete real value. \n",
      "* Probability Density Function -- Used when input feature is continuous. It answers what is the probability of X being in the neighborhood of x. \n",
      "\n",
      "\n",
      "##1.2.2 What Is a Variance-Covariance or Dispersion Matrix ##\n",
      "\n",
      "As shown in the image below, consider the following three distributions of two random variables X (say height) and Y (say weight).  While in the first plot as X increases Y increases, whereas in the third plot X and Y have inverse relationship. In the second plot there is no apparent relationships between the two variables. Let's say we tasked to capture the nature of the three distirbution and represent by a single numerical value that is greater than 0 for plot 1, equal to 0 for plot 2 and less than 0 for plot 3. \n",
      "\n",
      "![](images/chp1/cor1.png)\n",
      "\n",
      "One way to do this is move the origin of the axes in the three plots to the mean of the two random variables (this is show by intersection to two straight lines). Now all the data \n",
      "are split in four regions. If we take product x and y values in each of the four quadrants it will be positive in quadrant 1 and 3 and negative in quadrant 2 and 4. \n",
      "\n",
      "Further if we take mean of these products i.e $\\frac{1}{n}\\sum_{i=1}^{n}(x_i-\\mu_X)*(y_i-\\mu_Y)$, then it will > 0 for plot 1 as there more values in quadrant 1 and 3 then in quadrant 2 and 4. Similarly for plot 2 it will be around 0 and for plot 3 it will be less than 0. This particular numerical representation of relationship between two random variable is termed as covariance i.e. \n",
      "\n",
      "$$cov(X,Y) = \\frac{1}{n}\\sum_{i=1}^{n}(x_i-\\mu_X)*(y_i-\\mu_Y)$$\n",
      "\n",
      "Note that the covariance of a random variable to itself is same as variance i.e. $cov(X,X) = \\sigma^2$.  \n",
      "\n",
      "Now assume we have n dimensional euclidean space i.e point is represented by a vector of size n. If We take covariance of each dimension to all the other dimension we will get a $nXn$ **symmetric matrix** where diagonal elements represents variance. This matrix is referred as variance-covariance matrix and is denoted by $\\Sigma$. \n",
      "\n",
      "$$\\Sigma = \\begin{pmatrix}\n",
      " cov(x_1, x_1) & cov(x_1,x_2) & \\cdots  & cov(x_1,x_n)\\\\ \n",
      " cov(x_2,x_1) & cov(x_2,x_2) & \\cdots & cov(x_2, x_n)\\\\ \n",
      "\\cdots  & \\cdots  & \\cdots & cov(x_n,x_n) \\\\\n",
      "\\end{pmatrix}$$\n",
      "\n",
      "Variance-covariance matrix has the following properties:\n",
      "\n",
      "1. **Square**: It is a square matrix i.e. number of rows = number of columns\n",
      "2. **Symmetric**: Values across diagonals are same. i.e val(1,3) = val(3,1) \n",
      "3. **Non negative Definite**:  Euclidean distance between two points in a n-dimensional space is given as $d(x,y)=\\sqrt{\\sum_{i=1}^{n}(x_i - y_i)^2}$. However in this form euclidean distance is sensitive to scale of the dimension. For instance, con sider two people whose height and weight are initially measured in meter and kg. Based on the above equation we can calculate certain distance between these two people. However this euclidean distance will be very different if we measure height and weight in cm and kg. This is somewhat counterintuitive as the two persons are same. To overcome this problem, above euclidean distance can be generalized by adding weight (w) to each dimension as shown below: \n",
      "\n",
      "$$d(x,y)=\\sqrt{\\sum_{i=1}^{n}w_i(x_i - y_i)^2}$$\n",
      "\n",
      "We can further rewrite above equation in matrix form as show below\n",
      "\n",
      "$$d(x,y)= \\begin{pmatrix}\n",
      "x_1-y_1 &  x_2-y_2& \\cdot & x_n-y_n\n",
      "\\end{pmatrix} \\begin{pmatrix}\n",
      "w_1 & 0 & \\cdot & 0 \\\\ \n",
      "0 & w_2 & 0 & 0 \\\\ \n",
      "\\cdot & \\cdot & \\cdot & \\cdot \\\\ \n",
      "0 & 0 & \\cdot & w_n \n",
      "\\end{pmatrix}\\begin{pmatrix}\n",
      "x_1-y_1 \\\\\n",
      "x_2-y_2 \\\\\n",
      "\\cdot \\\\\n",
      "x_n-y_n\n",
      "\\end{pmatrix}$$\n",
      "\n",
      "We can further generalize above linear algebra by replacing all 0 with appropriate weights:\n",
      "\n",
      "$$d(x,y)= \\begin{pmatrix}\n",
      "x_1-y_1 &  x_2-y_2& \\cdot & x_n-y_n\n",
      "\\end{pmatrix} \\begin{pmatrix}\n",
      "w_{11} & w_{12} & \\cdot & w_{1n} \\\\ \n",
      "w_{21} & w_{22} & \\cdot & w_{2n}\\\\ \n",
      "\\cdot & \\cdot & \\cdot & \\cdot \\\\ \n",
      "w_{n1} & w_{n2} & \\cdot & w_{nn} \n",
      "\\end{pmatrix}\\begin{pmatrix}\n",
      "x_1-y_1 \\\\\n",
      "x_2-y_2 \\\\\n",
      "\\cdot \\\\\n",
      "x_n-y_n\n",
      "\\end{pmatrix}$$\n",
      "\n",
      "Now by definition distance is always $\\geq$ 0. Thus any weight matrix $w$ that gives above distance $>$ 0 is called **positive definite**. Theoretically **positive definite** is defined as any square matrix A for which\n",
      "\t\n",
      "$$A = Positive~Definite ==> if a^TAa > 0 ~~~\\forall a \\neq \\begin{pmatrix}\n",
      "0 \\\\ \n",
      "0 \\\\ \n",
      "\\cdots\\\\ \n",
      "0 \n",
      "\\end{pmatrix}$$\n",
      "\n",
      "Similarly we can define **positive semi definite** matrix $A_{nXn}$ as a matrix for which $a^TAa \\geq 0 ~~\\forall a$. Positive semi definite matrix is also known as **non negative matrix**. An important characteristic of positive definite matrix is that all its eigen value are positive. This property is further useful in Gaussian distribution.\n",
      "\n",
      "Note: Positive definite by itself indicate that the matrix is symmetric and symmetric metric by itself indicates that the matrix is square. \n",
      "\n",
      "\n",
      "\n",
      "## Explain Gaussian Distribution Function  ##\n",
      "\n",
      "Gaussian distribution function is a probability density function (pdf)that has the following form:\n",
      "\n",
      "$$pdf(x) = N(x|\\mu, \\Sigma) = \\frac{1}{(\\sqrt{2\\pi})^n|\\Sigma|^\\frac{1}{2}}exp\\{-\\frac{1}{2}(x-\\mu)^T\\Sigma^{-1}(x-\\mu)\\} ~~~\\forall ~~~x~\\epsilon~\\mathbb{R}^n$$\n",
      "\n",
      "where:\n",
      "\n",
      "* $\\mu$ is mean vector\n",
      "* $\\Sigma$ is a variance-covariance matrix or **dispersion matrix**. It is $nXn$ matrix \n",
      "* $|\\Sigma|$ is a determinant of variance-covariance matrix. \n",
      "\n",
      "There is however one particular challenge with the above equation. By definition probability density function should return value greater than 0. However determinant of a matrix is not necessarily always positive i.e. its possible $|\\Sigma| \\leq 0$. If this is true then $N(\\mu, \\Sigma)$ is either negative or undefined. In either case we break the conditional requirement of probability density function. Thus we need to guarantee that $\\Sigma$ is always positive. However determinant of a square matrix can be show to be equal to product of its eigen values. Thus if we can guarantee that all the eigen values are positive then $\\Sigma$ will be greater than zero. Above we leant that the variance-covariance matrix is non-negative definite. However in the case of Gaussian distribution it can be show that variance-covariance is positive definite and thus all its eigen values are greater than zero. As a result we are certain that $|\\Sigma| > 0$.\n",
      "\n",
      "**Empirical Rule or 68-95-99.7% Rule**\n",
      "* 68% of the observations fall within one standard deviation from the mean\n",
      "* 95% of the observations fall within two standard deviation from the mean\n",
      "* 99.7% of the observations fall within three standard deviations from the mean. \n",
      "\n",
      "If $\\mu=0$ and $\\sigma=1$ then the normal distribution is called **standard normal distribution** or the **unit normal distribution**\n",
      "\n",
      "###Why Gaussian distribution is important\n",
      "* lot of distributions (binomial, laplace, poisson) can be generalized to gaussian distribution\n",
      "* erros \n",
      "\n",
      "\n",
      "## Standardization ##\n",
      "One way to standardize variables is \n",
      "$$y=\\frac{x-\\mu}{\\sigma}$$\n",
      "\n",
      "1. If you do standardization then variance-covariance matrix becomes correlation matrix i.e. all the diagonal elements will be 1 and non-diagonal matrix will be correlation between those two features. \n",
      "2. You don't need to do standardization everytime. For instances places where we want variances to be preserved at that time you don't want to do standardization. For example when two classes measured for the same feature exhibit different variance i.e. one has large variance and another has a small variance that time you want to preserve variance information. \n",
      "\n",
      "## Normalization ##\n",
      "\n",
      "Different ways:\n",
      "$$y_i = \\frac{x_i - Min(X)}{Max(X)-Min(X)}$$\n",
      "\n",
      "all the values are in the interval of 0 to 1 (whereas in standardization values can be where but mean is 0 and variance 1). \n",
      "\n",
      "\n",
      "## Mahalanobis Distance\n",
      "Square distance (hyper-dismensional) of X from $\\mu_i$ weighted by $\\Sigma_{-1}$ is known as Mahalaanobis distance\n",
      "\t\n",
      "$$d_i = (X-\\mu_i)^T\\Sigma_{-1}(X-\\mu_i)$$\n",
      "\n",
      "Below are different curves showing influence of variance covariance matrices. \n",
      "\n",
      "\t\n",
      "if $\\Sigma_{-1} = \\mathbb{I}$ then criteria becomes euclidean distance norm\n"
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "##1.2 Parameter Estimation"
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "In the above example we assumed hieghts are normally distributed for male and female classes and that we can estimate distribution parameters namely mean ($\\mu$) and standard deviation ($\\sigma$) using the following equations:\n",
      "\n",
      "$$ \\mu = \\frac{\\Sigma{x_i}}{N}$$\n",
      "$$ \\sigma^2 = \\frac{\\Sigma{(x_i-\\mu)^2}}{N}$$\n",
      "\n",
      "As below discussed, the above method to compute normal distribution parameter is just one of the many ways. Specifically the above formula is derived from using **Maximum Likelihood Estimate**. There are other approaches to computing distribution parameters such as Maximum Apriori Probability (MAP), Bayesian Estimate, Unbiased Estimate, etc. Below we discuss few of the important approaches for parameter estimation. "
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "###1.2.1 Maximum Likelihood Estimation (MLE)\n",
      "Conside you want to estimate its probability of getting head $p$ of a coin. In order to to this you carried a coin toss experiment where you tossed the coin 20 ($n=20$) times and observed that 13 ($k=13$) times it landed on head. From our school days we know then the probability of getting head ($p$) is $k/n = 13/20=0.65$. But one might wonder how we arrived at $p = n/k$ formula. That's were MLE comes in picture. \n",
      "\n",
      "As per MLE, **we should select distribution parameter $\\theta$ such that it maximizes the probability of getting the dataset $\\mathcal{X}$ **. In reference to the above coin toss problem we should $p$ such that it maximizes the probability of getting 13 heads. Since coin toss experiment can be model as a binomial distribution we can write the probability of getting 13 heads as \n",
      "\n",
      "$$ P(\\mathcal{X}|p) = \\binom{n}{k}p^k(1-p)^{n-k} $$\n",
      "\n",
      "Thus our choice of $p$ based on MLE approach can be represented as:\n",
      "\n",
      "$$ \\widehat{p}_{ML} = \\arg\\max_p  P(\\mathcal{X}|p) $$\n",
      "\n",
      "However instead of directly maximizing $P(\\mathcal{X}|p)$ we often maximize $log P(\\mathcal{X}|p) $ to make solutions numerically stable (more on this later). Thus the problem to solve is\n",
      "\n",
      "$$ \\widehat{p}_{ML} = \\arg\\max_p log P(\\mathcal{X}|p) $$\n",
      "\n",
      "$$\\therefore \\widehat{p}_{ML} := \\frac{\\partial \\left[ log P(\\mathcal{X}|p) \\right]}{\\partial p} = 0 $$\n",
      "\n",
      "$$\\therefore \\frac{\\partial \\left[ log \\left( \\binom{n}{k}p^k(1-p)^{n-k} \\right) \\right]}{\\partial p} = 0 $$\n",
      "\n",
      "$$\\therefore \\frac{\\partial \\left[ log \\binom{n}{k} + k \\cdot log(p) + (n-k) \\cdot log(1-p)\\right]}{\\partial p} = 0 $$\n",
      "\n",
      "$$\\therefore \\frac{k}{p} - \\frac{(n-k)}{1-p} = 0 $$\n",
      "\n",
      "$$\\therefore \\widehat{p}_{ML} := p = \\frac{k}{n} $$\n",
      "\n",
      "As show in the plot below, for the given coin toss experiment we can empirically show that the above $p$ value maximizes the probability of getting 13 heads. "
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "%matplotlib inline\n",
      "import matplotlib.pyplot as plt\n",
      "import numpy as np\n",
      "from scipy.stats import binom\n",
      "import math\n",
      "\n",
      "n = 20  # Number of coin toss\n",
      "k = 13  # Number of success i.e. number of time we got head\n",
      "\n",
      "x = np.linspace(0,1,100) # Generate x axis \n",
      "plt.plot(x, binom.pmf(13, 20, x)) # Calculate probability of getting 13 head for different p values \n",
      "plt.axvline(13.0/20.0, linestyle=\"dashed\", color=\"black\") # Plot P as per MLE\n",
      "plt.show()"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "metadata": {},
       "output_type": "display_data",
       "png": "iVBORw0KGgoAAAANSUhEUgAAAX0AAAEACAYAAABfxaZOAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz\nAAALEgAACxIB0t1+/AAAIABJREFUeJzt3XmYVNWd//H3lwZGMSwujCKIQGREo4AaQMWlUdQWFxJ0\nJG5J3EJiiMbJJMZMRmCS/IyP6Ggeo1Ef4qAYwSiyuBEZacQdRhYRWjZRQFkk4o4sfX5/nO5Q6TRd\nVV1V99x76/N6nnrs6qpb9eXa9e3T33vO95hzDhERKQ8tQgcgIiLRUdIXESkjSvoiImVESV9EpIwo\n6YuIlBElfRGRMpI16ZtZlZnVmNlyM7u+kccvNrOFZrbIzF40s965HisiItGypubpm1kF8BYwGFgH\nzAUudM4tzXjOccAS59xHZlYFjHbOHZvLsSIiEq1sI/3+wArn3Grn3HZgIjA08wnOuZedcx/V3X0V\n6JLrsSIiEq1sSb8zsCbj/tq67+3OFcBTzTxWRERKrGWWx3Pu0WBmg4DLgYH5HisiItHIlvTXAQdl\n3D8IP2L/O3UXb+8DqpxzH+Z5rH45iIg0g3PO8j0mW3lnHtDTzLqZWWtgODAt8wlm1hWYDFzinFuR\nz7EZgevmHKNGjQoeQ1xuOhfFPRdpOZ9p+XcU49ZcTSZ959wOYCQwA1gCTHLOLTWzEWY2ou5pNwJ7\nA3eb2Xwze62pY5sdqYg025gxY0KHIDGRrbyDc+5p4OkG37sn4+srgStzPVZERMLRitwYqaysDB1C\nbOhc7KJzsYvOReGaXJwVSQBmLnQMImlnZgXVgSV+6v6fFv1CroiIpIiSvkgZGDVqVOgQJCZU3hER\nSSCVd0REJCslfRGRMqKkLyJSRpT0RUTKiJK+SBkYPXp06BAkJjR7R6QMaHFW+mj2joiIZJW14ZqI\nJNO2bfD001A/wF+2DLp1g9atg4YlgSnpi6TMRx/BvffCHXfAV78KHTr47595JrRoAffcA6ecEjZG\nCUflHZEUmTwZevSABQtg+nSYPRumTvWPrVwJt90G3/0uXHYZbN4cNFQJRElfJCUefhiuvhqefRYe\negiOOmrXY/W9d845B958E9q1g7594d13AwUrwWj2jkgKjB8PN9wAf/kLHHFEbsfceis88AC88AK0\nbVva+KT4mjt7R0lfJOHGj4df/tKP8Hv1yv045+B734MNG+Dxx6GionQxSvEp6YuUoZoaOOEEP1rP\nJ+HX27YNqqrg6KNh7Njixyelo3n6ImVm+3a49FL41a+al/DBT9989FGYNg0ee6y48Uk8aaQvklCj\nR8Orr8JTT4HlPd77e9XVflbP0qWw555FCE5KTiN9kTIydy7cfTeMG5dbws/We6eyEvr1g1tuKUp4\nEmMa6YskzLZt0KePH+kPH57bMbn03nnnHTjmGHj9dejatfA4pbQ00hcpE/feC927557wc3XwwfDD\nH8LPflbc15V40UhfJEE+/RR69vQ9dfr2zf24XLtsfv45HHYYTJgAJ55YQKBSchrpi5SB22+HQYPy\nS/j5aNMGbroJfv7z0ry+hKeRvkhCfPCBn5r5yitwyCH5HZtPP/2dO/1fE3/6Exx7bDMClUhopC+S\ncr/9LVxwQf4JH3b13slFRQVce61v0yDpo5G+SAKsWeNLOosXQ6dOpX+/Tz7xF4vnzvX/lfjRSF8k\nxW69Fa64IpqED74B2xVX+J78ki4a6YvE3Ecf+dH2okXQpUt077t2LfTuDatW7dqIReJDI32RlBo3\nzjdFizLhg3+/s87y6wIkPTTSF4mxHTv8hds//9m3SYja/Pl+45W334ZWraJ/f9k9jfRFUmjKFD/i\nLjThZ+u9sztHHeVbMsyYUdj7S3xopC8SYwMHwnXXwfnnF/Y6+czTb+iee2DmTP/XhsSHNlERSZnX\nXvPz8lesgJYtC3utQpL+li3QrZu/oLvPPoXFIcWj8o5Iytx+O1xzTeEJv1AdOsAZZ8CkSWHjkOLQ\nSF8khj74wF/AXb26ONMlCxnpg9+o5b/+y7eAkHjQSF8kRSZMgLPPjs/8+NNP9/3233ordCRSKCV9\nkZhxDv74R78itljy6b3TmJYt4eKLYfz4IgUkwai8IxIzc+fCt74Fy5dDixgNy954A4YM8SWniorQ\n0YjKOyIpMW4cXHZZvBI+wJFHQseOMGtW6EikEDH7sRIpb59/Do88At/9buhIGnfxxZrFk3RK+iIx\n8uijfuOSqPvs5GrYMJg61W+0IsmkpC8SI8W+gFts3bv7X0hz5oSORJpLSV8kJlauhCVLfIOzYmtu\n753GnHceTJ5ctJeTiGn2jkhM/PrXsH493Hln8V+70MVZmZYuhdNOg3ffjd/F5nJSstk7ZlZlZjVm\nttzMrm/k8V5m9rKZbTWznzR4bLWZLTKz+Wb2Wr7BiZQL5+Chh+Cii0JHkt1hh0G7dr43kCRPk0nf\nzCqAO4Eq4HDgQjM7rMHTNgM/AsY28hIOqHTOHeWc61+EeEVSaeFC2LoVjjsudCS5GTZMJZ6kyjbS\n7w+scM6tds5tByYCQzOf4Jzb5JybB2zfzWvk/eeHSLl5+GG/IMsS8mk57zx47DH/F4okS7ak3xlY\nk3F/bd33cuWAmWY2z8yuyjc4kXJQW+uTfhJKO/X69vVxL1oUOhLJV7amrYX+Hh/onHvfzDoCz5pZ\njXPuHyZ7Zc4sqKyspLKyssC3FUmOF1+E9u39itdSKbT3TkNmu0b7ffoU9aVlN6qrq6muri74dZqc\nvWNmxwKjnXNVdfdvAGqdczc38txRwKfOuVt381qNPq7ZO1Lurr7az33/xS9CR5Kfl16CESN8Tx6J\nXqlm78wDeppZNzNrDQwHpu0uhgYBtTGztnVf7wWcDujHQyTD9u1+G8ILLwwdSf4GDID33/dTNyU5\nmkz6zrkdwEhgBrAEmOScW2pmI8xsBICZHWBma4DrgF+a2btm9hXgAGCOmS0AXgWecM79pZT/GJGk\nefZZv1lK9+6hI8lfRQVUVcHTT4eORPKhxVkiAX3nO3DMMX5bxCR6+GGYONH345FoaWN0kYTZtg0O\nOMDXxDvnMycuRjZvhh49YONG+Kd/Ch1NeVE/fZGEee456NUrmoRfzN47mfbdF444AmbPLsnLSwko\n6YsE8uijcP750bzXmDFjSvbaQ4b4jdMlGVTeEQlgxw7o1AnmzYODDy79+xWz4VpD8+fD8OGwbFlJ\nXl52Q+UdkQSZPRu6dYsm4Zda377w6ad+T1+JPyV9kQCiLO2Umpkv8WjqZjIo6YtEbOdOePxx38Yg\nLVTXTw4lfZGIvfiin6p5yCHRvWexe+80NHiw/3d99llJ30aKQElfJGIhSjulmrJZr107+PrXoQj9\nwKTElPRFIlRb6zcfSVNpp95pp8HMmaGjkGyU9EUi9H//B23b+i0H0+a003wvIYk3JX2RCE2ZAt/4\nRugoSuPoo33XzffeCx2JNEVJXyRCaU76FRUwaJBKPHGnpC8SkWXL4MMPoV+/6N+71Bdy66nEE39q\nwyASkVtugVWr4O67o3/vUrZhyLRqFQwc6Es8SdnkPanUhkEk5qZMgaFDQ0dRWj16QJs28OaboSOR\n3VHSF4nA+vU+EQ4aFDqS0lOJJ96U9EUiMH2631qwHDYaUdKPNyV9kQikedZOQ4MGwQsvwJdfho5E\nGqOkL1Jin3wCc+bAmWeGi6HUvXcy7bOP3xHslVcie0vJg5K+SInNmAHHHQft24eLIaopm/VU4okv\nJX2REps2Lf2zdho69VS/B7DEj+bpi5TQjh2+jfL8+XDQQaGjic4XX0DHjr4tQ9u2oaNJJ83TF4mh\nl16Crl3LK+ED7LmnX3n8wguhI5GGlPRFSmjaNDj33NBRhDFokEo8caSkL1IizsHUqfFI+lFfyAU4\n5RSYNSvyt5UsVNMXKZGaGr+N4Jo14fvQRNV7J9O2bbDvvvDuu7D33pG+dVlQTV8kZupLO6ETfiit\nW8Pxx8Pzz4eORDIp6YuUSDnX8+sNGqQST9wo6YuUwKZNsHhxeTRYa8opp+hibtwo6YuUwJNP+lWp\n5dBgrSlHH+1r+ps2hY5E6inpi5TAtGlwzjmho9glyt47mVq2hBNOgOrqIG8vjdDsHZEi27oV9t8f\nVq6E/fYLHU14t90GK1bAXXeFjiRdNHtHJCZmzYLevZXw62mRVrwo6YsU2fTp8SrthNanD2zc6Pvw\nSHhK+iJF5JxP+uU+VTNTixZw0kkwe3boSASU9EWKasEC2GMPOPTQ0JHES2WlLubGhZK+SBHFdRVu\niN47mZT040Ozd0SK6Otfh7FjfZKLkxC9dzLV1voL22++CZ06BQsjVTR7RySwdetg1SoYODB0JPFT\nX9dXH57wlPRFiuSJJ/zm561ahY4knlTiiQclfZEi0VTNpp18spJ+HKimL1IEn33ma9XvvgsdOoSO\n5h+FrukD7Nzp6/pLl/p9g6UwqumLBDRzJvTvH8+ED+F672SqqNB8/ThQ0hcpgrj3zg89ZbOe6vrh\nKemLFGjnTn8RV/X87JT0w1PSFynQa6/5rprdu4eOJP5694YNG/xNwsia9M2sysxqzGy5mV3fyOO9\nzOxlM9tqZj/J51iRNIhb7/w4q6iAE0/UaD+kJpO+mVUAdwJVwOHAhWZ2WIOnbQZ+BIxtxrEiiRf3\nen7cnHyyLuaGlG2k3x9Y4Zxb7ZzbDkwEhmY+wTm3yTk3D9ie77EiSbdiBWzeDP36hY6kaXG5kAuq\n64eWLel3BtZk3F9b971cFHKsSCLUL8hqEfOrY2PGjAkdwt/06eN762/cGDqS8tQyy+OFrObI+djM\nUUhlZSWVcetWJbIb06fDddeFjiJZKir8vrmzZ8O//mvoaJKjurqa6iL8idTkilwzOxYY7Zyrqrt/\nA1DrnLu5keeOAj51zt2az7FakStJ9eGHcPDBsH49tGkTOpqmxWFFbqZbb4W334Y77wwdSXKVakXu\nPKCnmXUzs9bAcGDa7mIo4FiRxHn6ab//a9wTfhypD084TZZ3nHM7zGwkMAOoAMY555aa2Yi6x+8x\nswOAuUA7oNbMrgUOd8592tixpfzHiERpyhTN2mmuvn1h7VrYtAk6dgwdTXlRwzWRZvjyS78ga/ny\nZCSt0aNHx2oGD8BZZ8Hll8N554WOJJnUcE0kQs89B0cemYyED/GasllPUzfDUNIXaYYpU+Ab3wgd\nRbJVVmqRVggq74jkqbYWOneGOXPgkENCR5NcO3bAvvvCypW+z77kR+UdkYi89ppPVkr4hWnZ0u8n\nrH1zo6WkL5InlXaKR314oqekL5KnJCb9OF7IBV/XnzUrdBTlRTV9kTzU1MDgwbBmDVje1dRw4rYi\nt57q+s2nmr5IBKZOhaFDk5Xw46y+rq8ST3SU9EXyMGWKT/pSPIMGqcQTJSV9kRytXQvLlvk6tBTP\noEFapBUlJX2RHE2ZAmefDa1bh44kXer78Ki/fjSU9EVy9NhjMGxY6CiaZ9SoUaFD2K2WLbVvbpQ0\ne0ckB5s2+cVY69fDnnuGjiZ9brvNbz15112hI0kOzd4RKaGpU+GMM5TwS0Xz9aOjpC+Sg8mT1QK4\nlPr0gQ0b/N65UlpK+iJZbNkCL7wAQ4aEjiS9KirgpJNU14+Ckr5IFk8+6csPbduGjiTd1F8/Gkr6\nIlk89ljySztx7b2TSYu0oqHZOyJN+OwzOPBAePtt2Gef0NE0X1x772SqrYV//mdYsAC6dAkdTfxp\n9o5ICTz1FAwYkOyEnxQtWvgSz3PPhY4k3ZT0RZrwyCMwfHjoKMrH4MHwv/8bOop0U3lHZDc+/dRv\ni5j00g4ko7wDsHy5r+0nrXV1CCrviBTZE0/4tr9JT/hJcsghvsyzbFnoSNJLSV9kNyZNggsuCB1F\nccS5904mMzj1VJg5M3Qk6aXyjkgjPv4YDjoI3nkHOnQIHU15mTDBr4CePDl0JPGm8o5IEU2b5leI\nKuFH75RT/CKtnTtDR5JOSvoijZg0SbN2QjnwQOjUCebPDx1JOinpizSwZQs8/zyce27oSMrXqadq\n6mapKOmLNDBlii8xtGsXOpLypaRfOkr6Ig08/HD6SjtJ6L2T6eST4eWXYevW0JGkj2bviGR4/304\n/HBYtw7atAkdTfEkZXFWpgED4Le/9Yu15B9p9o5IEUycCEOHpivhJ9XgwZqvXwpK+iIZJkyASy4J\nHYUAnH46zJgROor0UXlHpM6SJX50uWaN38kpTZJY3tm2DTp29Bumd+wYOpr4UXlHpEAPPQQXXZS+\nhJ9UrVv7ev6zz4aOJF2U9EXwG3g89FB6SztJ6b3T0BlnqMRTbCrviOA3Pv/+9+GNN9TSN05WrYLj\nj4f33vPdN2UXlXdEClB/AVcJP1569PAb0i9aFDqS9FDSl7K3dSs8+qiv50v8qMRTXEr6UvamTIG+\nfaFr19CRSGOU9ItLSV/K3h//CFdcEToK2Z1Bg2DuXL99pRROSV/K2jvvwOuvwze/GTqS0kpa751M\nX/kK9OsHs2aFjiQdNHtHytro0fDBB3DnnaEjKa0kLs7KdPPNftFc2v8/5aO5s3daliIYkSTYuRPu\nv9/X9CXeqqrgvPPAOc2wKpTKO1K2nnsO9t0XjjoqdCSSTe/e8OWX8NZboSNJPiV9KVvjxukCblKY\nwdln+72LpTBK+lKWNm+GZ57R3PwkOeccmD49dBTJlzXpm1mVmdWY2XIzu343z/ld3eMLzeyojO+v\nNrNFZjbfzF4rZuAihXjwQRgyBPbeO3Qk0Uhq751Mp5wCCxf6X9jSfE3O3jGzCuAtYDCwDpgLXOic\nW5rxnCHASOfcEDMbANzhnDu27rG3gWOcc39t4j00e0ciVVsLvXr5+fknnBA6GsnHN78Jw4bBpZeG\njiS8UvXe6Q+scM6tds5tByYCQxs851xgPIBz7lWgg5ntnxlbvkGJlNLMmbDnnjBwYOhIJF8q8RQu\nW9LvDKzJuL+27nu5PscBM81snpldVUigIsXy+9/DyJGa+pdEZ53l++tv2xY6kuTKNk8/17rL7j4+\nJzjn3jOzjsCzZlbjnJvT8EmZqwUrKyuprKzM8W1F8rN6tW+j/Kc/hY5EmmP//eHQQ+H55/0uZ+Wk\nurqa6urqgl8nW03/WGC0c66q7v4NQK1z7uaM5/wBqHbOTay7XwOc7Jzb0OC1RgGfOudubfB91fQl\nMj//uZ/v/d//HToSaa7f/AY2boQ77ggdSVilqunPA3qaWTczaw0MBxrOlJ0GfLsuiGOBLc65DWbW\nxsza1n1/L+B04I18AxQplq1b/cXbq68OHUn0ktx7p6FzzvHz9TVWbJ6svXfM7EzgdqACGOecu8nM\nRgA45+6pe86dQBXwGXCZc+51M+sBTK57mZbAQ865mxp5fY30JRIPPOC3RCzHNr1J772TyTno3t1f\n0D3yyNDRhNPckb4arklZcM53arzxRjj33NDRRC9NSR/guuugfXvfMK9cabtEkSZUV8Mnn/il/JJ8\nF1wAf/5z6CiSSUlfysItt8BPf6rNtdNiwAD4+GN4883QkSSPPgKSem+8AfPn+43PJR1atIDzz9do\nvzmU9CX1xo6Fa66BPfYIHUk4aei905BKPM2jC7mSamvXQp8+sGJF+TRXKxe1tXDwwb5b6te+Fjqa\n6OlCrkgjbr8dvvMdJfw0UomneTTSl9TasgW++lVfz+/aNXQ0UgovvwxXXlmeF3Q10hdp4LbbYOhQ\nJfw00yye/GmkL6m0ebNvzDV3rl+9Kel13XXQrh2MGRM6kmhppC+SYexYX+9VwvfS1HunoYsuggkT\n1IsnVxrpS+ps3AiHHaZafqa0tWHI5JzvwXPXXXDSSaGjiY5G+iJ1br4ZLrxQCb9cmMFll8H994eO\nJBk00pdUef99P2d78WI48MDQ0cRHmkf6AOvX+7/u1qyBr3wldDTR0EhfBPjVr/y8fCX88nLAAXDi\niZqznwuN9CU1Fi+GQYOgpgb23Td0NPGS9pE+wOOP+8V4s2eHjiQaGulLWXPOT937z/9Uwm9MGnvv\nNHTWWbB0KaxcGTqSeNNIX1LhiSd86+RFi6BVq9DRSCg//jG0bevLfGmnnbOkbG3b5qfs3X47nHlm\n6GgkpIUL/R66b78NFRWhoyktlXekbN11F/TooYQvvqNq585+43RpnEb6kmjvvec/6LNnw+GHh45G\n4mDSJD8QSPsFXY30pew4B1dfDT/4gRK+7DJsmC/vvP566EjiSUlfEuvRR2HZMviP/wgdSfylufdO\nQ61awciR/hqP/COVdySRNm+GI46AyZPhuONCRxN/5TBPP9OHH/rrPEuWQKdOoaMpDZV3pKz827/5\nPVKV8KUxe+/tu2/edVfoSOJHI31JnOnT/Ubnb7xRPn1WClVuI32At97yrRneeQf23DN0NMWnkb6U\nhbVr4aqr4MEHlfClaYceCv37w/jxoSOJF430JTF27PC9dYYMgRtuCB1NspTjSB/gtdf8bJ7ly9M3\n2tdIX1JvzBj/wb3++tCRJE859N5pTP/+0K8f3H136EjiQyN9SYSZM33L5Ndfh/33Dx2NJMnixXDq\nqX60365d6GiKRyN9Sa3ly+GSS3wdXwlf8nXEEXD66Zq3X08jfYm1zZvh2GPhZz/zF3BFmmPlShgw\nwM/oSUvrbXXZlNT58ksYPBiOP97veytSiB/8APbaC8aODR1JcSjpS6rU1sKll/rE/8gj0EKFSCnQ\n++9D794wa5Yv+SSdavqSGrW1MGKEX1Tz4INK+MVQTr13dqdTJ/j1r+HKK2HnztDRhKORvsRKfcKv\nqYGnnvK7IEnhynWefkO1tX6tx7BhcO21oaMpjMo7knhK+KWjpL/LsmX+OtG8edCtW+homk/lHUm0\nL77wDbKWLYOnn1bCl9L5l3+Bn/zEDzDK8fegkr4Et369/5O7RQuYMUM9daT0/v3f/XTg224LHUn0\nlPQlqIUL/fzpIUPgoYdgjz1CRyTloFUrvxfD2LF+oFFOlPQlCOfg97/3y+NvvhluvBEs7+qk5Kpc\ne+80pWtXPx340kt9WbFc6EKuRG79erj8cti0CSZM8C1wRUK57z649VZ49VVo3z50NLnThVyJvdpa\n+J//gb594Zhj4KWXlPAlvKuugtNOg7PPhi1bQkdTehrpSyRefdXvdmUGv/udb3krEhe1tfDjH8Oc\nOfDMM8lo7KeRvsTSokXwrW/5xTA//KEf3SvhS9y0aAF33AFDh+7aYjGtlPSl6JyDF17wfy5XVflS\nTk0NfPvbaqkg8WUGo0fDyJEwcGB6Z/XoIyhFs2GDvyB2xBFw2WU+6a9aBT/9qRZbhabeO7m75hp/\n7el73/MLuD75JHRExZU16ZtZlZnVmNlyM2t0ozoz+13d4wvN7Kh8jpVke/ddP/XyjDOgVy+/S9Ef\n/uCnwH3/+5p3HxdjxowJHUKiDB7sS5M7dvjOnI88kp4mbU0mfTOrAO4EqoDDgQvN7LAGzxkCHOKc\n6wl8D7g712Pl71VXV4cOIat162DiRF+fP/JIX7qZO9ePiNasgfvv9zXRQufcJ+FcREXnYpcoz0X7\n9jBuHNx7r6/39+rlv966NbIQSiLbSL8/sMI5t9o5tx2YCAxt8JxzgfEAzrlXgQ5mdkCOx0qGuHy4\nnYONG/1F1wce8BuRV1XBAQdAnz4+6ffo4T8Q69f7P4WHDStu+4S4nIs40LnYJcS5OO00ePFFP6CZ\nOhUOPBAuvhgmTYKPP448nIK1zPJ4Z2BNxv21wIAcntMZODCHY6XEdu6Ezz/3t88+g48+2nX761/h\ngw/8bcMGP4pft86P2Fu3hkMO8bdevfzIvm9f6NJFK2elPJ1wAjz5JLz3HkyfDuPHwxVX+AFQ377+\n1rMndO7sPyf77RfPiQvZkn6uE+gLSgPnnFPI0fGQz1ID53Y9P/O45cvh5Zd3PV5/q63d9d/aWp/I\n6287dvjb9u27btu2+T9Bv/zS32/Txt/22sv/yVp/22cf/4PZsaNfJNWli/+B7dwZOnQo7vkRSYsD\nD/TlzBEj/OdsyRJYsADmz4eZM3cNnrZs8X/9tmvnJzLssYcfTLVuDS1bQkWF/6XQosXfD6QaDqqK\nPchqcnGWmR0LjHbOVdXdvwGodc7dnPGcPwDVzrmJdfdrgJOB7tmOrfu+VmaJiDRDcxZnZRvpzwN6\nmlk34D1gOHBhg+dMA0YCE+t+SWxxzm0ws805HNusoEVEpHmaTPrOuR1mNhKYAVQA45xzS81sRN3j\n9zjnnjKzIWa2AvgMuKypY0v5jxERkaYF770jIiLRiezaciGLvNIm27kws4vrzsEiM3vRzHqHiDMK\nuS7gM7N+ZrbDzIZFGV+UcvyMVJrZfDNbbGbVEYcYmRw+I/uZ2TNmtqDuXHw3QJglZ2Z/NLMNZvZG\nE8/JL28650p+w5d3VgDdgFbAAuCwBs8ZAjxV9/UA4JUoYov6luO5OA5oX/d1VTmfi4znPQc8AZwX\nOu6APxcdgDeBLnX39wsdd8BzMRq4qf48AJuBlqFjL8G5OBE4CnhjN4/nnTejGuk3d5FXAhqc5i3r\nuXDOveyc+6ju7qtAl4hjjEquC/h+BDwKbIoyuIjlci4uAh5zzq0FcM59EHGMUcnlXLwPtKv7uh2w\n2Tm3I8IYI+GcmwN82MRT8s6bUSX93S3gyvacNCa7XM5FpiuAp0oaUThZz4WZdcZ/4O+u+1ZaL0Ll\n8nPRE9jHzGaZ2TwzuzSy6KKVy7m4D/iamb0HLASujSi2uMk7b2abslkszV3klcYPeM7/JjMbBFwO\nDCxdOEHlci5uB37unHNmZhS4EDDGcjkXrYCjgVOBNsDLZvaKc255SSOLXi7n4hfAAudcpZl9FXjW\nzPo451LWEzMneeXNqJL+OuCgjPsH4X8jNfWcLnXfS5tczgV1F2/vA6qcc039eZdkuZyLY/BrQMDX\nbs80s+3OuWnRhBiZXM7FGuAD59wXwBdm9jzQB0hb0s/lXBwP/AbAObfSzN4GDsWvLSoneefNqMo7\nf1vkZWat8Qu1Gn5opwHfhr+tBN7inNsQUXxRynouzKwrMBm4xDm3IkCMUcl6LpxzPZxz3Z1z3fF1\n/R+kMOGqZbseAAAAuklEQVRDbp+RqcAJZlZhZm3wF+6WRBxnFHI5FzXAYIC6GvahwKpIo4yHvPNm\nJCN9V8Air7TJ5VwANwJ7A3fXjXC3O+dSt8lgjueiLOT4Gakxs2eARUAtcJ9zLnVJP8efi/8H3G9m\nC/GD15855/4aLOgSMbOH8W1t9jOzNcAofJmv2XlTi7NERMpIDBt/iohIqSjpi4iUESV9EZEyoqQv\nIlJGlPRFRMqIkr6ISBlR0hcRKSNK+iIiZeT/A5CnPtOhk6mBAAAAAElFTkSuQmCC\n",
       "text": [
        "<matplotlib.figure.Figure at 0x109b28f90>"
       ]
      }
     ],
     "prompt_number": 17
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "###1.2.2 Maximum A Posterior (MAP)\n",
      "In MLE we are purely driven by the data and completely ignore any prior knowledge. For instance we typically know that a fair coin has 0.5 probability of getting head. However this didn't play any role when estimating $p$. MAP tries to overcome this problem. It achieves this by inverting the problem. **Instead of maximizing $P(\\mathcal{X}|p)$, it tries to maximize $P(p|\\mathcal{X})$ and further apples bayes' formula to incorporate prior knowledge**:\n",
      "\n",
      "$$ P(p|\\mathcal{X})~=\\frac{~P(\\mathcal{X}|p)~\\times~P(p)}{P(\\mathcal{X})}$$\n",
      "\n",
      "Thus as per MAP we can \n",
      "\n",
      "$$ p_{MAP} = \\arg\\max_p  P(\\mathcal{X}|p) = \\arg\\max_p \\left[ \\frac{P(\\mathcal{X}|p)~\\times~P(p)}{P(\\mathcal{X})} \\right]$$\n",
      "\n",
      "$$ p_{MAP} := \\frac{\\partial \\left[ P(\\mathcal{X}|p)~\\times~P(p) \\right]}{\\partial p} = 0 $$\n",
      "\n",
      "However once challenge is how we express our prior belief. In the case of coin toss we know that a fair coin has a probability of getting head is 0.5. But we would like to have some variance around it to capture our uncertainity. Thus often we express our prior belief as a distribution itself. Theoretically we are free to choose any distribution but pratically computing the derivative can be difficult. Hence it is found that one choose a distribution for prior ($P(p)$) that has the same form as of $P(p|\\mathcal{x})$ then the whole optimization equation becomes much simpler and easier to solve. Such distribution of prior is then known as **conjugate prior**. For bernoulli distribution, the conjugate prior is provided by **beta distribution**. Beta distribution has two parameters and its probability mass function is given as:\n",
      "\n",
      "$$ P(p) = \\frac{1}{B(\\alpha, \\beta)}{p^{\\alpha-1}}{(1-p)^{\\beta-1}}$$\n",
      "\n",
      "For different values of $\\alpha~and~\\beta$, we get different shapes of beta distribution each of which peaks at $B(\\alpha,\\beta)=\\frac{\\Gamma(\\alpha)\\Gamma(\\beta)}{\\Gamma(\\alpha+\\beta)}$.Thus to model our belief about $p$ (= 0.5), we can experiment with different values of $\\alpha~and~\\beta$ and find where $P(p = 0.5)$  maximizes. For now, I just tell this will happen when $\\alpha=\\beta=5$. Thus, we can rewrite our Bayesian tranformation as\n",
      "\n",
      "$$P(p|X) = \\frac{P(X|p)\\frac{1}{B(\\alpha, \\beta)}{p^{\\alpha-1}}{(1-p)^{\\beta-1}}}{P(\\mathcal{X})}$$\n",
      "\n",
      "For the optimization purpose we can simplify the above equation in two ways. First we can get rid of the denominator as it is only for normalization purpose. Second we take the log sum instead of product to avoid the number overflow problem.\n",
      "\n",
      "$$log P(p|X) = log \\left[ P(\\mathcal{X}|p) \\right] + log \\left[ \\frac{1}{B(\\alpha, \\beta)} \\right] + log \\left( p^{\\alpha-1} \\right) + log \\left[ (1-p)^{\\beta-1} \\right]$$\n",
      "\n",
      "$$log P(p|X) = log \\left( \\binom{n}{k}p^k(1-p)^(k-n) \\right)  + log \\left[ \\frac{1}{B(\\alpha, \\beta)} \\right] + log \\left( p^{\\alpha-1} \\right) + log \\left[ (1-p)^{\\beta-1} \\right]$$\n",
      "\n",
      "$$log P(p|X) = log \\binom{n}{k} + k \\cdot log(p) + (n-k) \\cdot log(1-p) + log \\frac{1}{B(\\alpha, \\beta)} + log p^{\\alpha-1} + log (1-p)^{\\beta-1}$$\n",
      "\n",
      "Now to estimate $p$ we take the first derivative and equate it to zero i.e.\n",
      "\n",
      "$$ p_{MAP} := \\frac{\\partial \\left( log \\binom{n}{k} + k \\cdot log(p) + (n-k) \\cdot log(1-p) + log \\frac{1}{B(\\alpha, \\beta)} + log p^{\\alpha-1} + log (1-p)^{\\beta-1} \\right) }{\\partial p} = 0$$\n",
      "\n",
      "$$\\therefore p_{MAP} := \\frac{k}{p} - \\frac{n-k}{1-p} + \\frac{\\alpha -1}{p} - \\frac{\\beta-1}{1-p} = 0$$\n",
      "$$\\therefore p_{MAP} := p = \\frac{k+\\alpha-1}{n+\\alpha+\\beta-2}$$\n",
      "\n",
      "Plugin $n,k,\\alpha,\\beta$ from our coin toss experiment we get p\n",
      "\n",
      "$$ p = \\frac{13 + 5 - 1}{20 + 5 + 5 - 2} = \\frac{17}{28} = 0.6 $$\n",
      "\n",
      "In the case of MLE we obtained p = 0.65 but in the case of MAP, p = 0.6. This is because of the prior pulls the estimation towards it. The stronger our belief is the more is the pull towards the priori. In the case of beta distribution we can use $\\alpha, \\beta$ to express our confidence in the prior. The large the value for $\\alpha, \\beta$ (but equal) the stronger our belief that p is 0.5. In this case $\\alpha, \\beta$ are known as **hyperparameters**. \n",
      "\n",
      "On the other hand, the bigger the size of our dataset (n), the strong will be the pull towards an estimate driven by data. If n is sufficiently large then MAP and MLE estimate will converge. "
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "### 1.2.3 Bayesian Estimate\n",
      "Both MAP and MLE return point estimate i.e. one set of distribution paramaeter that maxmizes certain objective function. In contrast, bayesian estimate returns full posterior distribution $P(\\theta|\\mathcal{X})$. However the downside of the Bayesian approach is that it quickly becomes lot more difficult to find the closed form solution. Further the challenges are different when trying to solve the above equation analytically and practically.\n",
      "\n",
      "Analytically the challenge is computing probability of evidence i.e.$P(\\mathcal{X})$. In MAP we were able to ignore $P(\\mathcal{X})$ as it didn't influence the optimum value of distribution parameter. But in the case of Bayesian estimate since we want to get full posterior distribution we need probability of evidence. Using Bayesian equation, $P(\\mathcal{X})$ can be represented as\n",
      "\n",
      "$$ P(\\mathcal{X}) =  \\int_{\\theta}P(D|\\theta)P(\\theta)d\\theta$$\n",
      "\n",
      "Solving the above integration can prove to be difficult. However if we assume prior distribution to be conjugate prior of the posterior distribution, i.e. posterior $P(\\theta|\\mathcal{X})$ and the prior distribution $P(\\theta)$ belong to the same class of distribution,  then solving the above equation becomes much simpler. For now, as shown below, let\u2019s assume that we represent our prior belief about $\\theta$ using beta distribution. Beta distribution has two parameters that controls the shape of the distribution: $\\alpha, \\beta$. of the Beta distribution allows us to model our belief that a fair coin has equal chances of getting head or tail.\n",
      "\n",
      "$$Beta(\\alpha, \\beta) = \\frac{1}{B(\\alpha,\\beta)}x^{\\alpha-1}(1-x)^{\\beta-1}$$\n",
      "\n",
      "By representing our prior using Beta distribution we can rewrite probability of evidence as\n",
      "\n",
      "$$P(\\mathcal{X}) = \\int_{0}^{1}\\left ( \\prod_{i=1}^NP(x_i|\\theta) \\right ) \\times \\left( \\frac{1}{B(\\alpha,\\beta)}\\theta^{\\alpha-1}(1-\\theta)^{\\beta-1} \\right ) d\\theta$$\n",
      "$$P(\\mathcal{X}) = \\int_{0}^{1}\\left ( \\theta^{n_H}\\times (1-\\theta)^{N-n_H} \\right ) \\times \\left( \\frac{1}{B(\\alpha,\\beta)}\\theta^{\\alpha-1}(1-\\theta)^{\\beta-1} \\right ) d\\theta$$\n",
      "$$P(\\mathcal{X}) = \\frac{1}{Beta(\\alpha, \\beta)}\\int_{0}^{1} \\left ( \\theta^{n_H+\\alpha-1}\\times (1-\\theta)^{N-n_H+\\beta-1} \\right)  d\\theta$$\n",
      "\n",
      "Solving the above integration turns out to be simple and results into a constant value that can be represented as Z. Thus we can rewrite the posterior distribution as\n",
      "\n",
      "$$P(\\theta|\\mathcal{X}) = \\frac{1}{Z}\\times\\left(\\theta^{n_H}(1-\\theta)^{N-n_H}\\right)\\times\\left(\\frac{1}{Beta(\\alpha,\\beta)}\\theta^{\\alpha-1}(1-\\theta)^{\\beta-1}\\right)$$\n",
      "$$P(\\theta|\\mathcal{X}) = \\frac{1}{ZBeta(\\alpha, \\beta)}\\times\\left(\\theta^{n_H+\\alpha-1}(1-\\theta)^{N-n_H+\\beta-1}\\right)  ~~~~~~ (2)$$\n",
      "\n",
      "Note that the above posterior $P(\\theta|\\mathcal{X})$ has the same form as that of prior distribution (i.e. beta distribution) but with different parameters. \n",
      "\n",
      "For posterior we got $\\alpha = n_H + \\alpha$ and $\\beta = N - n_H + \\beta$. **For the given likelihood distribution (here bernoulli) if the choice of the prior distribution (here beta) results in the same form of distribution for the posterior then the choice of prior distribution is known as conjugate prior**. For bernoulli distribution, the conjugate prior is given as beta distribution. For gaussian distribution, the conjugate prior is gaussian distribution itself. \n",
      "\n",
      "For the beta distribution, the expected value of parameter is given as\n",
      "\n",
      "$$E\\left[Beta(x|\\alpha, \\beta)\\right] = \\frac{\\alpha}{\\alpha + \\beta}  ~~~~~~ (3)$$\n",
      "\n",
      "Based on equation 2 and 3, we can calculate expected value of $\\theta$ as\n",
      "$$\\theta = \\frac{n_H + \\alpha}{\\alpha + \\beta + N} ~~~~ (4)$$\n",
      "\n",
      "Thus pluggin values from our coin toss experiment, we can say that the expected probability of getting head is \n",
      "\n",
      "$$p = \\frac{13 + 5}{5 + 5 + 20} = 0.6$$. \n",
      "\n"
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "**References**\n",
      "1. https://engineering.purdue.edu/kak/Tutorials/Trinity.pdf\n",
      "2. "
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [],
     "language": "python",
     "metadata": {},
     "outputs": []
    }
   ],
   "metadata": {}
  }
 ]
}